{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}body{display:flex;flex-direction:column;height:100vh;min-height:100vh}.main-content{margin:8rem auto;max-width:60rem;padding-left:1.5rem}@media (width <= 720px){.main-content{margin-top:4rem}}.h2{font-size:1.5rem;font-weight:500;line-height:2.25rem}@media (width <= 720px){.h2{font-size:1.25rem;line-height:1.5rem}}#challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+);background-repeat:no-repeat;background-size:contain;padding-left:34px}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}}</style><meta http-equiv=\"refresh\" content=\"390\"></head><body class=\"no-js\"><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><noscript><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></noscript></div></div><script>(function(){window._cf_chl_opt={cvId: '3',cZone: \"himalayas.app\",cType: 'managed',cRay: '925cae1dcb8c2101',cH: 'R.IGZrVewXj7RLcI6_Ynev8CkIrff4bw17Jati4gFJo-1742887587-1.2.1.1-e0mmPd8eIfnb3cpDtyoNRNnaZxZCf0Z0wQNhZIr_.vyhM1lyr8rfZ1JpJckdb5qv',cUPMDTk: \"\\/resumes?__cf_chl_tk=vdY0Ql2n1GGeJ8An.umAMg3oWRmVRWPhtvOG0_bw_QA-1742887587-1.0.1.1-qs3y_uiUjpJ9ZdOZNqTbMP85u9Mqo9_YK3tKfh9y8Bg\",cFPWv: 'b',cITimeS: '1742887587',cTTimeMs: '1000',cMTimeMs: '390000',cTplC: 0,cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/resumes?__cf_chl_f_tk=vdY0Ql2n1GGeJ8An.umAMg3oWRmVRWPhtvOG0_bw_QA-1742887587-1.0.1.1-qs3y_uiUjpJ9ZdOZNqTbMP85u9Mqo9_YK3tKfh9y8Bg\",md: \"5D0Wqef_3lL2_4GWNjmiy6Ek4y8b5E7WaA_E4LLAGzY-1742887587-1.2.1.1-y3ZzZkIeADrhfHlQFGLJ56s5N7wSs0pP6WONCZGCaaYZ9L3pFrmQb9mK0fOZX7dL.ip7fAf8jmsWY.gs_1WjN3cuNPW5sKNzDaMyNByziXdsf5FJFpDZ1dstZVgziYnLOLf61C1NxaB0hp3HbhBYlGCnCErQA5ynMK6jUfqLJty_O8gQdND0Ku5cZ4pPbcwrjdYD8bfQj8q0.auQLU1WO4FnPAl5YKHBkBankIF78ab7nvcFMsjVd_Q47PUMl5u3efSPv0jwPBNdELmzTXM1gTo4lw9VPbLfoChc7BqgXzNPtxDe_q05tjTCsON6W1A3FOdYpa9IiDTbpl_PN.T4V.Kal2pTRJIJ3FZLnViM3Q3.xENizSUBBy0mVMERAk1JTNbvSrQJz6ilQ3t9Zmgms.1wyuacZmceXrfsIjG3Mv.9FlIkfvqP_Z0MRogwFNzTOKh2aDKLL93jKw.BmL2of49qhQg02elJ7PdSxbx24Gi0E2z9ZTe5DDL02ZBM8AMJJ_ooeIlNjR6tXg16zCoZBrr7uijD8ooa.VXiR6LpmmJgmtjGkEVVEp6dhNXn1CFA2qnazgcm4YLRvFYOwLyx4Wb.ln4zdfbptiQBa5V6LOOhB3PP5828t1A7HHSTwzIyKNHP.V59H_fxrFiGMHqHUkgLhRmsziZML5_KStYjA8ZMXtyyfcUjWe1uULXf2kzP0hGqBrs1bb9EiZkCeWLN8HN0I7ibr4eA9JU0mdxunr2hDDJK1rbvCQ1DCmclog_l9wLu_a6jbNMj2db6pBZOrTkJeerkaCQAOJKc4As8OoPIB3LJcPZbBIQCZ5SseNWqIzX4nMCIZTl26gkBB4_QiGUtKRYFTXRGZ8SJziye5UUada4Koslo9uf4zwH_.ezwdKZoFv93p7sLTQqARNVOSHqJQTFyTS7xMuBUnFiiiSy4b_4AmdLm7De_0UZ63SCrOdzjuQ0P.8ebQ3FQlUGf9LkFdONpPCkY5a_gEyc7ZHiBw_jGGVPKx.gkxUZ2ObeWNmJzFsyrQUQ9emMjK7zLMA\",mdrd: \"pzEExljf8kFtoIldmVgUoViIVppkhz49wwXk_3jeOk4-1742887587-1.2.1.1-NZzi5Yq1ouauXRh.QxFrN4hDyVgsF_YJolXxGPLKU7kywMQ0lJoy6FCRyqTQW5nMrJ3amxmB7J2k4rtcTT8F__rxNDZmoAIHsQgLnkUplRww4NFqmYBNKKt9KRef6LfsW0WOIvA3XvpeHc2Of5sFPoZcjQwIwjJYHISwZ67DZQHNXOIw3fruD.ZV062LthqJ6NreY4DC_kV_SsEKB5d7LyU3A_pz3zQMThs8WnvuDCpgIXpdOlvyEF6LCc90.eDCLzmaLmM.OcjQiVMvAqTmSkgAIa8Tsd9qGbxcooFfPklhOqI5UYfVLj6xgl8KDcIwzRm934QU3pXX.akQyT22EhGXlrySB3cD2OYR42kN5OuZqqsuGikdy86wNpV1C6hn3YbTCMKUMwI6w9cxE4Ib.YH18gHdCQbwST7TanrGHxJ1FggLGvYAbb1EEGLejXCPLsAc_M_xeB8MTy1RJZjZ0uCBiLHyd6V14EnkpZ0Boy5mDwpg7_EQWweHgh0BHRANLpqG1Hve33KdfAS6_5b0X9VJg.Z8rKUTz05atwXjI9o5lNA7RLFbDLYTYLpwcrd3LZ2dF7T.5vkcyF7eEjyLS7yYn2wDKaA3dFPtIGAw050KhjfcXiLugDHv7r9MfAlE.NKi75Gmqdn2Vh2HFFNJ_3NS2aYS2sg2dUcm7tskmC08W1qlkR_Fm6HrhmT.TndblI1bLeOh4ul7HOHePNqV47XiuER9GXYo3slEgQA0ZQhrFvQsuTInr.2o0yhcn37x9ICqNn_Vh6w94.Vn2gHzjZAn0aECFcWGstPRFNFp.NoNPgIiyGDhkyaqhTl1.x0ilvC2dXY7aHq.Qb6VVPkWkwpzQw6v5XC2eQo4KAkecj.fk99P2fkUz7OM60s23ac.YcqcIx45wmthQmBgMnKqRKbhefoWHS.0HQa_3aYEkghQ7WpawIxyiz7sGnpLSdqJkNe_O8r7LvUJEK.I_NQuU67eoQ.Gp70wlZ2jonap1PIErPqwMktFPMGuSDUCaQFC3AgPiSOsVKjwkKFRh2il9IH_NjBHeuzgkNntC2j4IvW6LtNq.gkyEkXqW4FztC1zCeCnNMTBog5sU5JqMUAK_JMRbcCAC2mbUDzA6tNNbCPac4v5YzcLZ63aeOoWhs36Ve8u46uBXDT5OKVYK95vD_OzBEaHLY..sTltoZJmk78yPncrnZJ1xKStHm6X547J3ICueucz12lzzhX9mhK3FidvTxzGS4Zz5w_uoFbybR8mPeSPaJUPxsLWOZc_bXOYVhFYv0rNJUOFQWSnBFuAuty027IVdBGjywn9J8Bnn7xVjaS6QQa9zNLduZEAz7YUy00BgVvWAQNkqDkCfP3Yc7SE0WBQ7SSf3vXYo2m5_A8Ye_qNC74nLATZxf4jVckwKdByXIoiAWQ1YtDA5aksNneI0Jk6.Ddhzpx9xrn5o2n_iZf8UVmgnWkZDP7a7ZU5n8_NHF5cqr.OfYwwUt9R9Nks1OUrzU74xasN.b62YzlU7X7_9vP5ee7aQyByMMFrhQ0tdG0jHelq45dcl6aWboV5TonR_Jnh7bRGDMzUT3f3hT5_jeGQReFLFKhzHBBJkIE.tfgjUQNoVgq5z9Hm.mltfDVZ41bl9Qpj6.HKKbFY.DCYrRHhiZ_7_OF2pEE9eDwMqdxb1szYkDpUpQzqp.HlwkxWKQO5UHTwFAfKFiP9lxt43llzK3RQ5gjM7_mhkvjf7t7ggceLdqKfwgYyS4c3QnzL2LfAz6RJhXtIPRac2bm5VFqy8ogAZbETlwNe1nodN86ttPQS92mdTfExC.SXXq4oC8SMK6dnBryuECfOqDiQtAbOd9uBOqVFRDZ.Oa3fcx98COHNbtKjKW8._rsHqZhTFo.M6OKNlDHlxxooDqjq4GuVj7rxfQVTR._lfF72gcphsp6aa.ChONs9wDRFa5gRVSRUJb6.rwt67sbYDwSILp4wlOAqFM0iuPMM6QR9UDk3Lr8GNAnXqscwTLSFAj2ZhprcBSRkZmg9RGbrJ8wNfZv3IfViZvE2FNWf6Ao_T0l2h4END9d.URl9ZZClxCuD9xcdk6JGXcZD34nRATHFY8QGhpkZqim28AVZ1JnkMPGnDfVOV.EGKYLQJEZqw5QIV69jGNnfLDh.W_Dbcx_hWHDc2oB4wVqU9XhJXEYe7aZ1ZxHPw5sDJf_lHLwKCoN00iFBOgUq38xSp.6TtMfxIVU_cD9bzQw3z8l0O_LOzP1_LVTHDH0kojoca0G83eDr51tOKG_Ntf5ZcnJS_.mBrYeM4mKJ5hzfXdBtyS6ZKJ3LzRsQjq5y_EMI9dLXSBviT5HEN9ta8vlPWquASkR5r3rzbPAZJjTh7Mt6\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=925cae1dcb8c2101';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/resumes?__cf_chl_rt_tk=vdY0Ql2n1GGeJ8An.umAMg3oWRmVRWPhtvOG0_bw_QA-1742887587-1.0.1.1-qs3y_uiUjpJ9ZdOZNqTbMP85u9Mqo9_YK3tKfh9y8Bg\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());</script></body></html>\n",
      "['overnight-stocker' 'owner-operator' 'oxidation-engineer' ... 'zookeeper'\n",
      " 'zoology-professor' 'zumba-instructor']\n",
      "Scraping: overnight-stocker\n",
      "Scraping: owner-operator\n",
      "Scraping: oxidation-engineer\n",
      "Scraping: oxygen-therapist\n",
      "Scraping: oyster-boat-laborer\n",
      "Scraping: oyster-buyer\n",
      "Scraping: pacu-nurse\n",
      "Scraping: pc-support-specialist\n",
      "Scraping: pca\n",
      "Scraping: pmo-manager\n",
      "Scraping: pt\n",
      "Scraping: pv-installer\n",
      "Scraping: pv-panel-installer\n",
      "Scraping: package-designer\n",
      "Scraping: package-handler\n",
      "Scraping: packaging-designer\n",
      "Scraping: packaging-engineer\n",
      "Scraping: packaging-machine-operator\n",
      "Scraping: packaging-manager\n",
      "Scraping: packaging-mechanic\n",
      "Scraping: packaging-operator\n",
      "Scraping: packaging-specialist\n",
      "Scraping: packaging-supervisor\n",
      "Scraping: packaging-technician\n",
      "Scraping: packer\n",
      "Scraping: packing-line-operator\n",
      "Scraping: packing-machine-can-feeder\n",
      "Scraping: pacs-administrator\n",
      "Scraping: pacu-rn\n",
      "Scraping: paddock-judge\n",
      "Scraping: paediatrician\n",
      "Scraping: pageant-director\n",
      "Scraping: paint-line-operator\n",
      "Scraping: paint-technician\n",
      "Scraping: painter\n",
      "Scraping: paintings-conservator\n",
      "Scraping: paleology-professor\n",
      "Scraping: paleontologist\n",
      "Scraping: palliative-care-nurse-practitioner\n",
      "Scraping: panel-saw-operator\n",
      "Scraping: paper-conservator\n",
      "Scraping: paraeducator\n",
      "Scraping: paralegal\n",
      "Scraping: paralegal-assistant\n",
      "Scraping: paralegal-instructor\n",
      "Scraping: paramedic\n",
      "Scraping: paraprofessional\n",
      "Scraping: parcel-post-carrier\n",
      "Scraping: parcel-post-clerk\n",
      "Scraping: parcel-post-distribution-machine-operator\n",
      "Scraping: parcel-truck-driver\n",
      "Scraping: parent-educator\n",
      "Scraping: park-police\n",
      "Scraping: park-ranger\n",
      "Scraping: parking-attendant\n",
      "Scraping: parking-enforcement-officer\n",
      "Scraping: parking-line-painter\n",
      "Scraping: parking-lot-associate\n",
      "Scraping: parking-lot-attendant\n",
      "Scraping: parking-manager\n",
      "Scraping: parole-agent\n",
      "Scraping: parole-officer\n",
      "Scraping: parquet-floor-layer\n",
      "Scraping: parquet-floor-layer-s-helper\n",
      "Scraping: part-time-cashiers\n",
      "Scraping: part-time-sales-associate\n",
      "Scraping: partner\n",
      "Scraping: parts-clerk\n",
      "Scraping: parts-counter-clerk\n",
      "Scraping: parts-delivery-driver\n",
      "Scraping: parts-driver\n",
      "Scraping: parts-manager\n",
      "Scraping: parts-specialist\n",
      "Scraping: party-chief\n",
      "Scraping: passenger-agent\n",
      "Scraping: passenger-booking-clerk\n",
      "Scraping: passenger-car-conductor\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "Response ended prematurely",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1119\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;66;03m# Truncated at start of next chunk\u001b[39;00m\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse ended prematurely\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mProtocolError\u001b[0m: Response ended prematurely",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Respectful delay to avoid being blocked\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 80\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category_name \u001b[38;5;129;01min\u001b[39;00m job_categories:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m     cvs \u001b[38;5;241m=\u001b[39m \u001b[43mget_cv_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cv \u001b[38;5;129;01min\u001b[39;00m cvs:\n\u001b[0;32m     82\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow(cv)\n",
      "Cell \u001b[1;32mIn[15], line 49\u001b[0m, in \u001b[0;36mget_cv_data\u001b[1;34m(category_name)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch all text from CVs using Cloudscraper.\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://himalayas.app/resumes/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to access:\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cloudscraper\\__init__.py:259\u001b[0m, in \u001b[0;36mCloudScraper.request\u001b[1;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     (method, url, args, kwargs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequestPreHook(\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    248\u001b[0m         method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    252\u001b[0m     )\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Make the request via requests.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    258\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodeBrotli(\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperform_request(method, url, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    260\u001b[0m )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Debug the request via the Response object.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cloudscraper\\__init__.py:192\u001b[0m, in \u001b[0;36mCloudScraper.perform_request\u001b[1;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(CloudScraper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\84905\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: Response ended prematurely"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = \"https://himalayas.app\"  # Replace with actual base URL\n",
    "CATEGORY_URL = f\"{BASE_URL}/resumes\"  # Replace with actual category page URL\n",
    "\n",
    "response = requests.get(CATEGORY_URL)\n",
    "print(response.text)  # Check if the HTML contains expected elements\n",
    "\n",
    "# def get_job_categories():\n",
    "#     response = requests.get(CATEGORY_URL)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     job_categories = []\n",
    "#     print(soup.select(\"a.text-primary-700\"))\n",
    "#     for a_tag in soup.select(\"a.text-primary-700\"):  # More general selector\n",
    "#         print(a_tag)\n",
    "#         if 'href' in a_tag.attrs:\n",
    "#             job_categories.append((a_tag.text.strip(), BASE_URL + a_tag['href']))\n",
    "\n",
    "#     return job_categories\n",
    "\n",
    "# def get_cv_data(category_name):\n",
    "#     \"\"\"Fetch all text from CVs on a job category page.\"\"\"\n",
    "#     response = requests.get(f\"https://himalayas.app/resumes/{category_name}\")\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     print(soup.prettify())\n",
    "#     cvs = []\n",
    "#     for cv_div in soup.select(\"div.flex.h-full.w-full.flex-row.rounded-lg.bg-white\"):\n",
    "#         print(cv_div)\n",
    "#         text_content = \" \".join(cv_div.stripped_strings)  # Extract all text and clean up spaces\n",
    "#         cvs.append({\n",
    "#             \"Category\": category_name,\n",
    "#             \"Text\": text_content\n",
    "#         })\n",
    "    \n",
    "#     print(cvs)\n",
    "\n",
    "#     return cvs\n",
    "\n",
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()  # Mimics a real browser\n",
    "\n",
    "def get_cv_data(category_name):\n",
    "    \"\"\"Fetch all text from CVs using Cloudscraper.\"\"\"\n",
    "    url = f\"https://himalayas.app/resumes/{category_name}\"\n",
    "    response = scraper.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to access:\", url)\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # print(soup.prettify())\n",
    "    \n",
    "    cvs = []\n",
    "    \n",
    "    for cv_div in soup.select(\"div.flex.h-full.w-full.flex-row.rounded-lg.bg-white\"):\n",
    "        text_content = \" \".join(cv_div.stripped_strings)\n",
    "        cvs.append({\"Category\": category_name, \"Text\": text_content})\n",
    "\n",
    "    # print(cvs)\n",
    "    return cvs\n",
    "\n",
    "\n",
    "file_path = \"job_category_clean.xlsx\"  \n",
    "def main():\n",
    "    job_categories = pd.read_excel(file_path, header=None, dtype=str).values[5500:].ravel()\n",
    "    print(job_categories)\n",
    "    \n",
    "    with open(\"cv_text_data10.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        fieldnames = [\"Category\", \"Text\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for category_name in job_categories:\n",
    "            print(f\"Scraping: {category_name}\")\n",
    "            cvs = get_cv_data(category_name)\n",
    "            for cv in cvs:\n",
    "                writer.writerow(cv)\n",
    "            time.sleep(2)  # Respectful delay to avoid being blocked\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been concatenated successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Create a pattern to match all CSV files\n",
    "file_pattern = \"cv_text_data*.csv\"\n",
    "\n",
    "# Get all files matching the pattern\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through all files and read them into dataframes\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optionally, save the concatenated dataframe to a new CSV\n",
    "concatenated_df.to_csv(\"concatenated_cv_text_data.csv\", index=False)\n",
    "\n",
    "print(\"Files have been concatenated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\84905\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\84905\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\84905\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"concatenated_cv_text_data.csv\")\n",
    "\n",
    "# Initialize necessary tools\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove the literal phrase \"Contact [email protected]\"\n",
    "    text = re.sub(r'Contact \\[email protected\\]', '', text)\n",
    "    \n",
    "    # Remove the bullet character '•'\n",
    "    text = text.replace('•', '')\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenization (split into words)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Text' column\n",
    "df['Processed_Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Optionally, remove rows with missing text\n",
    "df = df.dropna(subset=['Processed_Text'])\n",
    "\n",
    "# Save the preprocessed data to a new CSV\n",
    "df.to_csv(\"preprocessed_cv_text_data.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
